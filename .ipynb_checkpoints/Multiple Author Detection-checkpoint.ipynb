{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Author Detection Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from nltk import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DataAggregator(path):\n",
    "    text_dict = {}\n",
    "    json_dict = {}\n",
    "    label_df = pd.DataFrame(columns=['changes', 'positions','file'])\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.txt'):\n",
    "            with open(path+file, 'r', encoding='utf-8') as myfile:\n",
    "                text_dict[file.replace('problem-', '').replace('.txt', '')] = myfile.read().replace('\\n', '')\n",
    "        else:\n",
    "            with open(path+file, 'r', encoding='utf-8') as myfile:\n",
    "                data = json.load(myfile)\n",
    "                json_dict[file.replace('problem-', '').replace('.truth', '')] = data['changes']\n",
    "\n",
    "    df = pd.DataFrame(list(text_dict.items()), columns=['file', 'text'])\n",
    "    label_df = pd.DataFrame(list(json_dict.items()), columns = ['file', 'changes'])\n",
    "\n",
    "    output_df = df.merge(label_df,on='file')\n",
    "    output_df = output_df.drop('file', axis = 1)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the data sets to data frames\n",
    "test_df = DataAggregator('C:/Users/RSC/PyProjects/Data-Science/Data/pan18-style-change-detection-test-dataset-2018-01-31/')\n",
    "training_df = DataAggregator('C:/Users/RSC/PyProjects/Data-Science/Data/pan18-style-change-detection-training-dataset-2018-01-31/')\n",
    "validation_df = DataAggregator('C:/Users/RSC/PyProjects/Data-Science/Data/pan18-style-change-detection-validation-dataset-2018-01-31/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Text: 2980, Training Labels: 2980\n",
      "Test Text: 1352, Test Labels: 1352\n",
      "Validation Text: 1492, Validation Labels: 1492\n"
     ]
    }
   ],
   "source": [
    "#Split the data into text and labels\n",
    "training_labels = training_df['changes']\n",
    "validation_labels = validation_df['changes']\n",
    "test_labels = test_df['changes']\n",
    "\n",
    "training_text = training_df['text']\n",
    "validation_text = validation_df['text']\n",
    "test_text = test_df['text']\n",
    "\n",
    "print(\"Training Text: \" + str(len(training_text)) + \", Training Labels: \" + str(len(training_labels)))\n",
    "print(\"Test Text: \" + str(len(test_text)) + \", Test Labels: \" + str(len(test_labels)))\n",
    "print(\"Validation Text: \" + str(len(validation_text)) + \", Validation Labels: \" + str(len(validation_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Text: 4472, Training Labels: 4472\n"
     ]
    }
   ],
   "source": [
    "new_train_text = training_text.append(validation_text)\n",
    "new_train_labels = training_labels .append(validation_labels)\n",
    "print(\"Training Text: \" + str(len(new_train_text)) + \", Training Labels: \" + str(len(new_train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test ngram size\n",
    "\n",
    "ngrams = [(1,1),(1,2), (1,3), (1,4), (1,5), (1,6), (1,7), (1,8), (1,9), (1,10)]\n",
    "\n",
    "accuracy_count = []\n",
    "accuracy_tfidf = []\n",
    "\n",
    "clf = BernoulliNB()\n",
    "\n",
    "for ngram in ngrams:\n",
    "    X_train, X_test = train_test_split(training_df)\n",
    "    vec = CountVectorizer(strip_accents = 'ascii', ngram_range = ngram, analyzer = 'word', stop_words='english')\n",
    "    training_text = vec.fit_transform(X_train['text'])\n",
    "    testing_text = vec.transform(X_test['text'])\n",
    "    \n",
    "    clf.fit(training_text, X_train['changes'])\n",
    "    pred = clf.predict(testing_text)\n",
    "    \n",
    "    accuracy_count.append(metrics.accuracy_score(pred, X_test['changes']))\n",
    "  \n",
    "\n",
    "for ngram in ngrams:\n",
    "    X_train, X_test = train_test_split(training_df)\n",
    "    vec = TfidfVectorizer(strip_accents = 'ascii', ngram_range = ngram, analyzer = 'word', stop_words='english')\n",
    "    training_text = vec.fit_transform(X_train['text'])\n",
    "    testing_text = vec.transform(X_test['text'])\n",
    "    \n",
    "    clf.fit(training_text, X_train['changes'])\n",
    "    pred = clf.predict(testing_text)\n",
    "    \n",
    "    accuracy_tfidf.append(metrics.accuracy_score(pred, X_test['changes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_count)\n",
    "print(accuracy_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "X_train, X_test = train_test_split(training_df)\n",
    "vec = TfidfVectorizer(tokenizer=stemmer.stem, strip_accents = 'ascii', ngram_range=(1,3), analyzer='word', stop_words='english')\n",
    "stemmed_train_text = vec.fit_transform(X_train['text'])\n",
    "stemmed_test_text = vec.transform(X_test['text'])\n",
    "\n",
    "clf = BernoulliNB()\n",
    "\n",
    "clf.fit(stemmed_train_text, X_train['changes'])\n",
    "pred = clf.predict(stemmed_test_text)\n",
    "\n",
    "print(metrics.accuracy_score(pred, X_test['changes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Builds a pipeline\n",
    "NB_pipeline = Pipeline([('vect', CountVectorizer(strip_accents = 'ascii', ngram_range=(1,3), analyzer='word', stop_words='english')),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf', BernoulliNB())])\n",
    "\n",
    "MNB_pipeline = Pipeline([('vect', CountVectorizer(strip_accents = 'ascii', ngram_range=(1,3), analyzer='word', stop_words='english')),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf', MultinomialNB())])\n",
    "\n",
    "SVC_pipeline = Pipeline([('vect', CountVectorizer(strip_accents = 'ascii', ngram_range=(1,3), analyzer='word', stop_words='english')),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SVC(C=0.5, probability=True))])\n",
    "\n",
    "KNN_pipeline = Pipeline([('vect', CountVectorizer(strip_accents = 'ascii', ngram_range=(1,3), analyzer='word', stop_words='english')),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', KNeighborsClassifier(10))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the models and fit to the validation set\n",
    "start = time.time()\n",
    "nb_proba = NB_pipeline.fit(new_train_text, new_train_labels).predict_proba(test_text)\n",
    "nb_prediction = NB_pipeline.predict(test_text)\n",
    "print(\"Bernoulli Naive Bayes Accuracy: \" + str(metrics.accuracy_score(nb_prediction, test_labels)) + \" -- Run time \" + str(time.time()-start))\n",
    "nb_confusion = metrics.confusion_matrix(test_labels, nb_prediction)\n",
    "print(nb_confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "mnb_proba = MNB_pipeline.fit(new_train_text, new_train_labels).predict_proba(test_text)\n",
    "mnb_prediction = MNB_pipeline.predict(test_text)\n",
    "print(\"Multinomial Naive Bayes Accuracy: \" + str(metrics.accuracy_score(mnb_prediction, test_labels)) + \" -- Run time \" + str(time.time()-start))\n",
    "mnb_confusion = metrics.confusion_matrix(test_labels, mnb_prediction)\n",
    "print(nb_confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "svc_proba = SVC_pipeline.fit(new_train_text, new_train_labels).predict_proba(test_text)\n",
    "svc_prediction = SVC_pipeline.predict(test_text)\n",
    "print(\"SVC Accuracy: \" + str(metrics.accuracy_score(svc_prediction, test_labels)) + \" -- Run time \" + str(time.time()-start))\n",
    "svc_confusion = metrics.confusion_matrix(test_labels, svc_prediction)\n",
    "print(svc_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "knn_proba = KNN_pipeline.fit(new_train_text, new_train_labels).predict_proba(test_text)\n",
    "knn_prediction = KNN_pipeline.predict(test_text)\n",
    "print(\"KNN Accuracy: \" + str(metrics.accuracy_score(knn_prediction, test_labels)) + \" -- Run time \" + str(time.time()-start))\n",
    "knn_confusion = metrics.confusion_matrix(test_labels, knn_prediction)\n",
    "print(knn_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['Naive Bayes(Bernoulli)', 'Naive Bayes(Multinomial)', 'K-Nearest Neighbor', 'Support Vector Classifier']\n",
    "\n",
    "nb_acc = metrics.accuracy_score(nb_prediction, test_labels)\n",
    "mnb_acc = metrics.accuracy_score(mnb_prediction, test_labels)\n",
    "knn_acc = metrics.accuracy_score(knn_prediction, test_labels)\n",
    "svc_acc = metrics.accuracy_score(svc_prediction, test_labels)\n",
    "\n",
    "values = [nb_acc, mnb_acc, knn_acc, svc_acc]\n",
    "\n",
    "fix, ax = plt.subplots(figsize=(7,6))\n",
    "ax.bar(np.arange(4), height=values, width = 0.3)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xticks(np.arange(4))\n",
    "ax.set_ylabel('Accuracy Score')\n",
    "ax.set_xlabel('Classifier')\n",
    "ax.set_ylim(0,1)\n",
    "\n",
    "for i, v in enumerate(values):\n",
    "    ax.text(i-.05, v+0.02, '%.3f' % v, fontweight ='bold')\n",
    "\n",
    "ax.set_title('Classifer Accuracy Results')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['TN', 'FP', 'FN', 'TP']\n",
    "\n",
    "confusion = [[nb_confusion[0][0], nb_confusion[0][1], nb_confusion[1][0], nb_confusion[1][1]],\n",
    "             [mnb_confusion[0][0], mnb_confusion[0][1], mnb_confusion[1][0], mnb_confusion[1][1]],\n",
    "       [knn_confusion[0][0], knn_confusion[0][1], knn_confusion[1][0], knn_confusion[1][1]],\n",
    "       [svc_confusion[0][0], svc_confusion[0][1], svc_confusion[1][0], svc_confusion[1][1]]]\n",
    "\n",
    "df = pd.DataFrame(data = confusion, columns = columns, index = labels)\n",
    "\n",
    "print(\"Confusion Matrix Summary\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(nb_predicition, test_labels))\n",
    "print(metrics.classification_report(mnb_predicition, test_labels))\n",
    "print(metrics.classification_report(knn_predicition, test_labels))\n",
    "print(metrics.classification_report(svc_predicition, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_fpr, nb_tpr, __ = metrics.roc_curve(test_labels, nb_proba[:,1])\n",
    "mnb_fpr, mnb_tpr, __ = metrics.roc_curve(test_labels, mnb_proba[:,1])\n",
    "svc_fpr, svc_tpr, __ = metrics.roc_curve(test_labels, svc_proba[:,1])\n",
    "knn_fpr, knn_tpr, __ = metrics.roc_curve(test_labels, knn_proba[:,1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.set_title(\"Receiver Operating Characteristic\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "ax.plot(nb_fpr, nb_tpr, color='darkorange', label='NB')\n",
    "plt.plot(svc_fpr, svc_tpr, color = 'red', label='SVC')\n",
    "plt.plot(knn_fpr, knn_tpr, color = 'green', label = 'KNN')\n",
    "ax.plot([0,1], [0,1], 'r--', color='navy')\n",
    "\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
